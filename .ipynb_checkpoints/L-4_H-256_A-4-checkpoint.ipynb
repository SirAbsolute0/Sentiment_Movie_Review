{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "884bd454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "O:\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"XLA_GPU:0\"\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, BertModel\n",
    "from transformers import InputExample, InputFeatures\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ac29699",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load train and test data into pandas df\n",
    "train_df = np.load('train/data.npz')\n",
    "test_df = np.load('test/test.npz')\n",
    "\n",
    "train = pd.DataFrame([train_df['a'], train_df['b']]).T\n",
    "train_data, train_vald_data = train_test_split(train, test_size = 0.2, shuffle = True, random_state = 1209)\n",
    "train_data.columns = ['DATA_COLUMN', 'LABEL_COLUMN']\n",
    "train_vald_data.columns = ['DATA_COLUMN', 'LABEL_COLUMN']\n",
    "\n",
    "test = pd.DataFrame([test_df['a'], test_df['b']]).T\n",
    "test.columns = ['DATA_COLUMN', 'LABEL_COLUMN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57bcde82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             DATA_COLUMN LABEL_COLUMN\n",
      "0      I found this movie really hard to sit through,...          1.0\n",
      "1      The movie starts off with Reeve (Ekin) and his...          1.0\n",
      "2      I had a VERY hard time sitting through this fi...          1.0\n",
      "3      I'm not a big fan of musicals, but I was alway...          0.0\n",
      "4      I honestly fail to understand why people love ...          1.0\n",
      "...                                                  ...          ...\n",
      "24995  I was surprised that I liked this movie. But i...          0.0\n",
      "24996  I saw this movie in 1956 and again on Cable a ...          1.0\n",
      "24997  This is a movie that was probably made to ente...          1.0\n",
      "24998  Okay, make no mistake - this is a pretty awful...          1.0\n",
      "24999  UK-born Australian helmer Alex Frayne calls fo...          0.0\n",
      "\n",
      "[25000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1517505f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN): \n",
    "  train_InputExamples = train_data.apply(lambda x: InputExample(guid=None, \n",
    "                                                          text_a = x[DATA_COLUMN], \n",
    "                                                          text_b = None,\n",
    "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "  validation_InputExamples = train_vald_data.apply(lambda x: InputExample(guid=None, \n",
    "                                                          text_a = x[DATA_COLUMN], \n",
    "                                                          text_b = None,\n",
    "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
    "  \n",
    "  return train_InputExamples, validation_InputExamples\n",
    "  \n",
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    features = [] # -> will hold InputFeatures to be converted later\n",
    "\n",
    "    for e in examples:\n",
    "        # Documentation is really strong for this method, so please take a look at it\n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            e.text_a,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length, # truncates if len(s) > max_length\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            pad_to_max_length=True, # pads to the right by default # CHECK THIS for pad_to_max_length\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\n",
    "            input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield (\n",
    "                {\n",
    "                    \"input_ids\": f.input_ids,\n",
    "                    \"attention_mask\": f.attention_mask,\n",
    "                    \"token_type_ids\": f.token_type_ids,\n",
    "                },\n",
    "                f.label,\n",
    "            )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.float64),\n",
    "        (\n",
    "            {\n",
    "                \"input_ids\": tf.TensorShape([None]),\n",
    "                \"attention_mask\": tf.TensorShape([None]),\n",
    "                \"token_type_ids\": tf.TensorShape([None]),\n",
    "            },\n",
    "            tf.TensorShape([]),\n",
    "        ),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84dd1565",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_COLUMN = 'DATA_COLUMN'\n",
    "LABEL_COLUMN = 'LABEL_COLUMN'\n",
    "\n",
    "train_InputExamples, validation_InputExamples = convert_data_to_examples(train_data, \n",
    "                                                                           train_vald_data, \n",
    "                                                                           DATA_COLUMN, \n",
    "                                                                           LABEL_COLUMN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2e02a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 383/383 [00:00<00:00, 383kB/s]\n",
      "Downloading: 100%|██████████| 43.0M/43.0M [00:18<00:00, 2.44MB/s]\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading: 100%|██████████| 226k/226k [00:00<00:00, 1.22MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained(\"google/bert_uncased_L-4_H-256_A-4\", from_pt=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"google/bert_uncased_L-4_H-256_A-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a020add5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  11170560  \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  514       \n",
      "=================================================================\n",
      "Total params: 11,171,074\n",
      "Trainable params: 11,171,074\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850a259c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "O:\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_InputExamples, validation_InputExamples = convert_data_to_examples(train_data, train_vald_data, DATA_COLUMN, LABEL_COLUMN)\n",
    "\n",
    "training_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
    "training_data = training_data.shuffle(100).batch(32).repeat(2)\n",
    "\n",
    "validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\n",
    "validation_data = validation_data.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5854a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
    "\n",
    "model.fit(training_data, epochs=1, validation_data=validation_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
